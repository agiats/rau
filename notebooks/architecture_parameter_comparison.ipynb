{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture Parameter Comparison\n",
    "\n",
    "This notebook compares parameter counts across different architectures:\n",
    "\n",
    "- LSTM\n",
    "- Transformer\n",
    "- Stack-RNN (with superposition)\n",
    "- Stack-Transformer\n",
    "\n",
    "We'll also determine optimal hyperparameters given a fixed parameter budget.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from typing import Dict, Optional\n",
    "\n",
    "# Set plotting style\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_transformer_params(vocab_size: int, num_layers: int, d_model: int,\n",
    "                                feedforward_size: int) -> int:\n",
    "    \"\"\"Calculate parameter count for transformer analytically.\"\"\"\n",
    "    # Embeddings (tied with output)\n",
    "    embedding_params = vocab_size * d_model\n",
    "\n",
    "    # Per layer: attention + feedforward + layer norms\n",
    "    per_layer = (\n",
    "        4 * d_model * d_model +  # Q, K, V, O projections (no bias)\n",
    "        d_model * feedforward_size + feedforward_size +  # FFN layer 1\n",
    "        feedforward_size * d_model + d_model +  # FFN layer 2\n",
    "        2 * 2 * d_model  # 2 Layer norms (weight and bias each)\n",
    "    )\n",
    "\n",
    "    # Final layer norm\n",
    "    final_params = 2 * d_model\n",
    "\n",
    "    return embedding_params + num_layers * per_layer + final_params\n",
    "\n",
    "\n",
    "def calculate_lstm_params(vocab_size: int, num_layers: int, hidden_units: int) -> int:\n",
    "    \"\"\"Calculate parameter count for LSTM analytically.\n",
    "\n",
    "    Based on PyTorch LSTM implementation without extra bias (use_extra_bias=False).\n",
    "    \"\"\"\n",
    "    # Embeddings\n",
    "    embedding_params = vocab_size * hidden_units\n",
    "\n",
    "    # Initial hidden states (only h for each layer, c starts at 0)\n",
    "    init_params = num_layers * hidden_units\n",
    "\n",
    "    # LSTM layers: 4 gates * (input weights + hidden weights)\n",
    "    # No bias terms when use_extra_bias=False\n",
    "    lstm_params = num_layers * 4 * (hidden_units * hidden_units + hidden_units * hidden_units)\n",
    "\n",
    "    # Output projection\n",
    "    output_params = hidden_units * vocab_size + vocab_size\n",
    "\n",
    "    return embedding_params + init_params + lstm_params + output_params\n",
    "\n",
    "\n",
    "def calculate_rnn_params(vocab_size: int, num_layers: int, hidden_units: int) -> int:\n",
    "    \"\"\"Calculate parameter count for simple RNN analytically.\"\"\"\n",
    "    # Embeddings\n",
    "    embedding_params = vocab_size * hidden_units\n",
    "\n",
    "    # Initial hidden states\n",
    "    init_params = num_layers * hidden_units\n",
    "\n",
    "    # RNN layers (input weights + hidden weights, no bias with use_extra_bias=False)\n",
    "    rnn_params = num_layers * (hidden_units * hidden_units + hidden_units * hidden_units)\n",
    "\n",
    "    # Output projection\n",
    "    output_params = hidden_units * vocab_size + vocab_size\n",
    "\n",
    "    return embedding_params + init_params + rnn_params + output_params\n",
    "\n",
    "\n",
    "def calculate_stack_rnn_params(vocab_size: int, num_layers: int, hidden_units: int,\n",
    "                             stack_size: int, controller: str = 'lstm') -> int:\n",
    "    \"\"\"Calculate parameter count for Stack-RNN with superposition analytically.\"\"\"\n",
    "    # Base RNN/LSTM parameters\n",
    "    if controller == 'lstm':\n",
    "        base_params = calculate_lstm_params(vocab_size, num_layers, hidden_units)\n",
    "    else:\n",
    "        base_params = calculate_rnn_params(vocab_size, num_layers, hidden_units)\n",
    "\n",
    "    # Stack-specific parameters (based on SuperpositionStackRNN)\n",
    "    # MultiLayer for actions: creates separate layers for each stack\n",
    "    # In this case, just one stack, so hidden_units -> 3 (with bias by default)\n",
    "    action_params = hidden_units * 3 + 3\n",
    "\n",
    "    # Push value layer: hidden_units -> stack_size with sigmoid (includes bias)\n",
    "    push_value_params = hidden_units * stack_size + stack_size\n",
    "\n",
    "    # Total stack overhead\n",
    "    stack_params = action_params + push_value_params\n",
    "\n",
    "    return base_params + stack_params\n",
    "\n",
    "\n",
    "def calculate_stack_transformer_params(vocab_size: int, num_layers: int, num_stack_layers: int,\n",
    "                                     d_model: int, feedforward_size: int, stack_size: int) -> int:\n",
    "    \"\"\"Calculate parameter count for Stack-Transformer with superposition analytically.\n",
    "\n",
    "    The architecture string format should be: d_model-num_regular.stack_type-stack_size.num_stack\n",
    "    For example: 768-2.superposition-32.2 means 2 regular layers + 2 stack layers\n",
    "    \"\"\"\n",
    "    # Embeddings (tied)\n",
    "    embedding_params = vocab_size * d_model\n",
    "\n",
    "    # Regular transformer layers\n",
    "    regular_per_layer = (\n",
    "        4 * d_model * d_model +  # Q, K, V, O projections\n",
    "        d_model * feedforward_size + feedforward_size +  # FFN layer 1\n",
    "        feedforward_size * d_model + d_model +  # FFN layer 2\n",
    "        2 * 2 * d_model  # Layer norms\n",
    "    )\n",
    "\n",
    "    # Stack attention layers (based on SuperpositionStackAttention)\n",
    "    stack_per_layer = (\n",
    "        # Stack attention specific parameters (no bias)\n",
    "        d_model * 3 +  # Action layer (no bias)\n",
    "        d_model * stack_size +  # Input to pushed vector (no bias)\n",
    "        stack_size * d_model +  # Stack reading to output (no bias)\n",
    "        # Regular transformer FFN and layer norms\n",
    "        d_model * feedforward_size + feedforward_size +  # FFN layer 1\n",
    "        feedforward_size * d_model + d_model +  # FFN layer 2\n",
    "        2 * 2 * d_model  # Layer norms\n",
    "    )\n",
    "\n",
    "    # Final layer norm\n",
    "    final_params = 2 * d_model\n",
    "\n",
    "    total_params = (\n",
    "        embedding_params +\n",
    "        num_layers * regular_per_layer +\n",
    "        num_stack_layers * stack_per_layer +\n",
    "        final_params\n",
    "    )\n",
    "\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Empirical Parameter Counts ===\n",
      "\n",
      "LSTM (h=32): 27,104 parameters\n",
      "LSTM (h=64): 103,360 parameters\n",
      "LSTM (h=128): 403,328 parameters\n",
      "Transformer (d=32): 40,224 parameters\n",
      "Transformer (d=48): 87,984 parameters\n",
      "Transformer (d=56): 118,776 parameters\n",
      "Transformer (d=64): 154,176 parameters\n",
      "Transformer (d=128): 603,264 parameters\n",
      "Stack-RNN (h=32, s=20): 30,423 parameters\n",
      "Stack-RNN (h=32, s=32): 32,355 parameters\n",
      "Stack-RNN (h=32, s=64): 37,507 parameters\n",
      "Stack-RNN (h=64, s=20): 109,975 parameters\n",
      "Stack-RNN (h=64, s=32): 113,827 parameters\n",
      "Stack-RNN (h=64, s=64): 124,099 parameters\n",
      "Stack-Transformer (d=16, s=16): 59,568 parameters\n",
      "Stack-Transformer (d=20, s=8): 111,340 parameters\n",
      "Stack-Transformer (d=20, s=16): 111,660 parameters\n",
      "Stack-Transformer (d=24, s=16): 187,848 parameters\n",
      "\n",
      "=== Parameter Summary ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Architecture</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Hidden/d_model</th>\n",
       "      <th>Stack Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM (h=32)</td>\n",
       "      <td>27104</td>\n",
       "      <td>32</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LSTM (h=64)</td>\n",
       "      <td>103360</td>\n",
       "      <td>64</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LSTM (h=128)</td>\n",
       "      <td>403328</td>\n",
       "      <td>128</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Transformer (d=32)</td>\n",
       "      <td>40224</td>\n",
       "      <td>32</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transformer (d=48)</td>\n",
       "      <td>87984</td>\n",
       "      <td>48</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Transformer (d=56)</td>\n",
       "      <td>118776</td>\n",
       "      <td>56</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Transformer (d=64)</td>\n",
       "      <td>154176</td>\n",
       "      <td>64</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Transformer (d=128)</td>\n",
       "      <td>603264</td>\n",
       "      <td>128</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Stack-RNN (h=32, s=20)</td>\n",
       "      <td>30423</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Stack-RNN (h=32, s=32)</td>\n",
       "      <td>32355</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Stack-RNN (h=32, s=64)</td>\n",
       "      <td>37507</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Stack-RNN (h=64, s=20)</td>\n",
       "      <td>109975</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Stack-RNN (h=64, s=32)</td>\n",
       "      <td>113827</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Stack-RNN (h=64, s=64)</td>\n",
       "      <td>124099</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Stack-Transformer (d=16, s=16)</td>\n",
       "      <td>59568</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Stack-Transformer (d=20, s=8)</td>\n",
       "      <td>111340</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Stack-Transformer (d=20, s=16)</td>\n",
       "      <td>111660</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Stack-Transformer (d=24, s=16)</td>\n",
       "      <td>187848</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Architecture  Parameters  Hidden/d_model Stack Size\n",
       "0                      LSTM (h=32)       27104              32          -\n",
       "1                      LSTM (h=64)      103360              64          -\n",
       "2                     LSTM (h=128)      403328             128          -\n",
       "3               Transformer (d=32)       40224              32          -\n",
       "4               Transformer (d=48)       87984              48          -\n",
       "5               Transformer (d=56)      118776              56          -\n",
       "6               Transformer (d=64)      154176              64          -\n",
       "7              Transformer (d=128)      603264             128          -\n",
       "8           Stack-RNN (h=32, s=20)       30423              32         20\n",
       "9           Stack-RNN (h=32, s=32)       32355              32         32\n",
       "10          Stack-RNN (h=32, s=64)       37507              32         64\n",
       "11          Stack-RNN (h=64, s=20)      109975              64         20\n",
       "12          Stack-RNN (h=64, s=32)      113827              64         32\n",
       "13          Stack-RNN (h=64, s=64)      124099              64         64\n",
       "14  Stack-Transformer (d=16, s=16)       59568              16         16\n",
       "15   Stack-Transformer (d=20, s=8)      111340              20          8\n",
       "16  Stack-Transformer (d=20, s=16)      111660              20         16\n",
       "17  Stack-Transformer (d=24, s=16)      187848              24         16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use empirical parameter counts instead\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Add the rau module to path\n",
    "sys.path.append('/Users/agiats/Projects/lm_inductive_bias/src/rau/src')\n",
    "\n",
    "from rau.models.transformer.unidirectional_encoder import get_unidirectional_transformer_encoder\n",
    "from rau.models.rnn.language_model import get_simple_rnn_language_model, get_lstm_language_model\n",
    "from rau.models.stack_nn.transformer.unidirectional_encoder import get_unidirectional_stack_transformer_encoder\n",
    "from rau.models.stack_nn.rnn.language_model import get_stack_rnn_language_model\n",
    "from rau.models.stack_nn.transformer.parse import parse_stack_transformer_layers\n",
    "from rau.models.stack_nn.rnn.parse import parse_stack_rnn_stack\n",
    "\n",
    "\n",
    "def count_parameters(model: nn.Module) -> int:\n",
    "    \"\"\"Count the number of trainable parameters in a model.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def get_empirical_params(architecture: str, vocab_size: int, **kwargs) -> int:\n",
    "    \"\"\"Get empirical parameter count for a given architecture.\"\"\"\n",
    "\n",
    "    if architecture == 'lstm':\n",
    "        model = get_lstm_language_model(\n",
    "            input_vocabulary_size=vocab_size,\n",
    "            output_vocabulary_size=vocab_size,\n",
    "            hidden_units=kwargs['hidden_units'],\n",
    "            layers=kwargs['num_layers'],\n",
    "            dropout=0.0,\n",
    "            learned_hidden_state=True,\n",
    "            use_padding=False\n",
    "        )\n",
    "\n",
    "    elif architecture == 'transformer':\n",
    "        model = get_unidirectional_transformer_encoder(\n",
    "            input_vocabulary_size=vocab_size,\n",
    "            output_vocabulary_size=vocab_size,\n",
    "            tie_embeddings=True,\n",
    "            num_layers=kwargs['num_layers'],\n",
    "            d_model=kwargs['d_model'],\n",
    "            num_heads=kwargs['num_heads'],\n",
    "            feedforward_size=kwargs['feedforward_size'],\n",
    "            dropout=0.0,\n",
    "            use_padding=False\n",
    "        )\n",
    "\n",
    "    elif architecture == 'stack-rnn':\n",
    "        stack_spec = f\"superposition-{kwargs['stack_size']}\"\n",
    "        stack = parse_stack_rnn_stack(stack_spec)\n",
    "        model = get_stack_rnn_language_model(\n",
    "            input_vocabulary_size=vocab_size,\n",
    "            output_vocabulary_size=vocab_size,\n",
    "            hidden_units=kwargs['hidden_units'],\n",
    "            layers=kwargs['num_layers'],\n",
    "            controller='lstm',\n",
    "            stack=stack,\n",
    "            dropout=0.0,\n",
    "            learned_hidden_state=True,\n",
    "            use_padding=False,\n",
    "            tag=None\n",
    "        )\n",
    "\n",
    "    elif architecture == 'stack-transformer':\n",
    "        # Format: d_model-num_regular.stack_type-stack_size.num_stack\n",
    "        layer_spec = f\"{kwargs['d_model']}-{kwargs['num_layers']}.superposition-{kwargs['stack_size']}.{kwargs['num_stack_layers']}\"\n",
    "        layers = parse_stack_transformer_layers(layer_spec)\n",
    "        model = get_unidirectional_stack_transformer_encoder(\n",
    "            input_vocabulary_size=vocab_size,\n",
    "            output_vocabulary_size=vocab_size,\n",
    "            tie_embeddings=True,\n",
    "            layers=layers,\n",
    "            d_model=kwargs['d_model'],\n",
    "            num_heads=kwargs['num_heads'],\n",
    "            feedforward_size=kwargs['feedforward_size'],\n",
    "            dropout=0.0,\n",
    "            use_padding=False\n",
    "        )\n",
    "\n",
    "    return count_parameters(model)\n",
    "\n",
    "\n",
    "# Test our empirical counting\n",
    "vocab_size = 64\n",
    "num_layers = 3\n",
    "\n",
    "print(\"=== Empirical Parameter Counts ===\\n\")\n",
    "\n",
    "# Test configurations\n",
    "configs = [\n",
    "    ('LSTM (h=32)', 'lstm', {'hidden_units': 32, 'num_layers': num_layers}),\n",
    "    ('LSTM (h=64)', 'lstm', {'hidden_units': 64, 'num_layers': num_layers}),\n",
    "    ('LSTM (h=128)', 'lstm', {'hidden_units': 128, 'num_layers': num_layers}),\n",
    "\n",
    "    ('Transformer (d=32)', 'transformer', {'d_model': 32, 'num_heads': 4, 'feedforward_size': 128, 'num_layers': num_layers}),\n",
    "     ('Transformer (d=48)', 'transformer', {'d_model': 48, 'num_heads': 4, 'feedforward_size': 192, 'num_layers': num_layers}),\n",
    "     ('Transformer (d=56)', 'transformer', {'d_model': 56, 'num_heads': 4, 'feedforward_size': 224, 'num_layers': num_layers}),\n",
    "    ('Transformer (d=64)', 'transformer', {'d_model': 64, 'num_heads': 4, 'feedforward_size': 256, 'num_layers': num_layers}),\n",
    "    ('Transformer (d=128)', 'transformer', {'d_model': 128, 'num_heads': 4, 'feedforward_size': 512, 'num_layers': num_layers}),\n",
    "\n",
    "    ('Stack-RNN (h=32, s=20)', 'stack-rnn', {'hidden_units': 32, 'stack_size': 20, 'num_layers': num_layers}),\n",
    "    ('Stack-RNN (h=32, s=32)', 'stack-rnn', {'hidden_units': 32, 'stack_size': 32, 'num_layers': num_layers}),\n",
    "    ('Stack-RNN (h=32, s=64)', 'stack-rnn', {'hidden_units': 32, 'stack_size': 64, 'num_layers': num_layers}),\n",
    "    ('Stack-RNN (h=64, s=20)', 'stack-rnn', {'hidden_units': 64, 'stack_size': 20, 'num_layers': num_layers}),\n",
    "    ('Stack-RNN (h=64, s=32)', 'stack-rnn', {'hidden_units': 64, 'stack_size': 32, 'num_layers': num_layers}),\n",
    "    ('Stack-RNN (h=64, s=64)', 'stack-rnn', {'hidden_units': 64, 'stack_size': 64, 'num_layers': num_layers}),\n",
    "\n",
    "    ('Stack-Transformer (d=16, s=16)', 'stack-transformer',\n",
    "     {'d_model': 16, 'num_heads': 4, 'feedforward_size': 64, 'stack_size': 16, 'num_layers': 1, 'num_stack_layers': 1}),\n",
    "    ('Stack-Transformer (d=20, s=8)', 'stack-transformer',\n",
    "     {'d_model': 20, 'num_heads': 4, 'feedforward_size': 80, 'stack_size': 8, 'num_layers': 1, 'num_stack_layers': 1}),\n",
    "    ('Stack-Transformer (d=20, s=16)', 'stack-transformer',\n",
    "     {'d_model': 20, 'num_heads': 4, 'feedforward_size': 80, 'stack_size': 16, 'num_layers': 1, 'num_stack_layers': 1}),\n",
    "    ('Stack-Transformer (d=24, s=16)', 'stack-transformer',\n",
    "     {'d_model': 24, 'num_heads': 4, 'feedforward_size': 96, 'stack_size': 16, 'num_layers': 1, 'num_stack_layers': 1}),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for name, arch, params in configs:\n",
    "    try:\n",
    "        param_count = get_empirical_params(arch, vocab_size, **params)\n",
    "        results.append({\n",
    "            'Architecture': name,\n",
    "            'Parameters': param_count,\n",
    "            'Hidden/d_model': params.get('hidden_units') or params.get('d_model'),\n",
    "            'Stack Size': params.get('stack_size', '-')\n",
    "        })\n",
    "        print(f\"{name}: {param_count:,} parameters\")\n",
    "    except Exception as e:\n",
    "        print(f\"{name}: Error - {e}\")\n",
    "\n",
    "df_empirical = pd.DataFrame(results)\n",
    "print(\"\\n=== Parameter Summary ===\")\n",
    "display(df_empirical)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
